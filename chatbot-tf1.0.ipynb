{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# ChatBot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Data収集\n",
    "日本語の学習用データを用意する。\n",
    "* [開発データ・評価データ - 対話破綻検出チャレンジ2](https://sites.google.com/site/dialoguebreakdowndetection2/downloads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "/home/tsu-nera/anaconda3/lib/python3.6/site-packages/zmq/backend/cython/../../../../.././libstdc++.so.6: version `GLIBCXX_3.4.20' not found (required by /usr/lib/libmecab.so.2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-90a6f8eb3217>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mMeCab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/tsu-nera/anaconda3/lib/python3.6/site-packages/MeCab.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_mod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0m_MeCab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tsu-nera/anaconda3/lib/python3.6/site-packages/MeCab.py\u001b[0m in \u001b[0;36mswig_import_helper\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfp\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                 \u001b[0m_mod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_MeCab'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tsu-nera/anaconda3/lib/python3.6/imp.py\u001b[0m in \u001b[0;36mload_module\u001b[0;34m(name, file, filename, details)\u001b[0m\n\u001b[1;32m    240\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mload_dynamic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mload_dynamic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mtype_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mPKG_DIRECTORY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_package\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tsu-nera/anaconda3/lib/python3.6/imp.py\u001b[0m in \u001b[0;36mload_dynamic\u001b[0;34m(name, path, file)\u001b[0m\n\u001b[1;32m    340\u001b[0m         spec = importlib.machinery.ModuleSpec(\n\u001b[1;32m    341\u001b[0m             name=name, loader=loader, origin=path)\n\u001b[0;32m--> 342\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: /home/tsu-nera/anaconda3/lib/python3.6/site-packages/zmq/backend/cython/../../../../.././libstdc++.so.6: version `GLIBCXX_3.4.20' not found (required by /usr/lib/libmecab.so.2)"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import MeCab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def load_json(filepath):\n",
    "    fj = open(filepath,'r')\n",
    "    json_data = json.load(fj)\n",
    "    fj.close()\n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def output(data, mode):\n",
    "    for i in range(len(data['turns'])):\n",
    "        if mode == \"U\" and data['turns'][i]['speaker'] == mode:\n",
    "            print(data['turns'][i]['utterance'])\n",
    "        elif mode == \"S\" and data['turns'][i]['speaker'] == mode and i != 0:\n",
    "            print(data['turns'][i]['utterance'])\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def output_with_pair(data):\n",
    "    for i in range(len(data['turns'])):\n",
    "        print(data['turns'][i][\"speaker\"] + \":\" + data['turns'][i]['utterance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data = load_json(\"DBDC2_dev/IRS/1471400435.log.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "output(data, \"U\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "output(data, \"S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "output_with_pair(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Mecab で 文章を分解する\n",
    "* [Ubuntu 16.04.1 LTSにPython 3(Anaconda)とMeCabをインストールする : 二日坊主な私](http://blueskydb.blog.jp/archives/67055421.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "m = MeCab.Tagger(\"-Owakati\")\n",
    "def get_text(data):\n",
    "    user = []\n",
    "    system = []\n",
    "    for i in range(len(data['turns'])):\n",
    "        if data['turns'][i]['speaker'] == \"U\":\n",
    "            user.append(m.parse(data['turns'][i]['utterance']))\n",
    "        elif data['turns'][i]['speaker'] == \"S\" and i != 0:\n",
    "            system.append(m.parse(data['turns'][i]['utterance']))\n",
    "        else:\n",
    "            continue\n",
    "    return user, system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### データをすべて読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "source_text = []\n",
    "target_text = []\n",
    "\n",
    "import glob\n",
    "for path in glob.glob('*/*/*.log.json'):\n",
    "    data = load_json(path)\n",
    "    user, system = get_text(data)\n",
    "    source_text.extend(user)\n",
    "    target_text.extend(system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "len(source_text), len(target_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Data収集その2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "source_text2 = []\n",
    "target_text2 = []\n",
    "m = MeCab.Tagger(\"-Owakati\")\n",
    "\n",
    "with open(\"sequence.txt\",'r') as f:\n",
    "    for row in f:\n",
    "        if row.startswith(\"input:\"):\n",
    "            data = row[7:]\n",
    "            data = m.parse(data)\n",
    "            source_text2.append(data)\n",
    "        else:\n",
    "            data = row[8:]\n",
    "            data = m.parse(data)\n",
    "            target_text2.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "source_text2 = source_text2[:10000]\n",
    "target_text2 = target_text2[:10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def text_to_ids(source_text, target_text, source_vocab_to_int, target_vocab_to_int):\n",
    "    eos = target_vocab_to_int['<EOS>']\n",
    "    source_id_text = [[source_vocab_to_int[word] for word in sequence.split()] \n",
    "                      for sequence in source_text]\n",
    "    target_id_text = [[target_vocab_to_int[word] for word in sequence.split()] + [eos] \n",
    "                      for sequence in target_text]\n",
    "    return source_id_text, target_id_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "CODES = {'<PAD>': 0, '<EOS>': 1, '<UNK>': 2, '<GO>': 3 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "def create_lookup_tables(text):\n",
    "    vocab = set()\n",
    "    for line in text:\n",
    "        for word in line.split(\" \"):\n",
    "                vocab.add(word)\n",
    "    \n",
    "    vocab_to_int = copy.copy(CODES)\n",
    "\n",
    "    for v_i, v in enumerate(vocab, len(CODES)):\n",
    "        vocab_to_int[v] = v_i\n",
    "\n",
    "    int_to_vocab = {v_i: v for v, v_i in vocab_to_int.items()}\n",
    "\n",
    "    return vocab_to_int, int_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "source_vocab_to_int, source_int_to_vocab = create_lookup_tables(source_text2)\n",
    "target_vocab_to_int, target_int_to_vocab = create_lookup_tables(target_text2)\n",
    "\n",
    "source_int_text, target_int_text = text_to_ids(source_text2, target_text2, \n",
    "                                               source_vocab_to_int, target_vocab_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6524, 5792)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(source_vocab_to_int), len(target_vocab_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Save Data\n",
    "with open('preprocess.p', 'wb') as out_file:\n",
    "    pickle.dump((\n",
    "        (source_text2, target_text2),\n",
    "        (source_vocab_to_int, target_vocab_to_int),\n",
    "        (source_int_to_vocab, target_int_to_vocab)), out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def load_preprocess():\n",
    "    with open('preprocess.p', mode='rb') as in_file:\n",
    "        return pickle.load(in_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Build the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def model_inputs():\n",
    "    inputs = tf.placeholder(tf.int32, [None, None], name=\"input\")\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name=\"target\")\n",
    "    learning_rate = tf.placeholder(tf.float32, name=\"learning_rate\")\n",
    "    keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "    return inputs, targets, learning_rate, keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_decoding_input(target_data, target_vocab_to_int, batch_size):\n",
    "    go = target_vocab_to_int['<GO>']\n",
    "    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
    "    dec_input = tf.concat([tf.fill([batch_size, 1], go), ending], 1)\n",
    "    return dec_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def encoding_layer(rnn_inputs, rnn_size, num_layers, keep_prob):\n",
    "    enc_cell = tf.contrib.rnn.MultiRNNCell(\n",
    "        [tf.contrib.rnn.BasicLSTMCell(rnn_size)] * num_layers)\n",
    "    enc_cell = tf.contrib.rnn.DropoutWrapper(enc_cell, output_keep_prob=keep_prob)\n",
    "    _, enc_state = tf.nn.dynamic_rnn(enc_cell, rnn_inputs, dtype=tf.float32)\n",
    "    return enc_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def decoding_layer_train(encoder_state, dec_cell, dec_embed_input, sequence_length,\n",
    "                         decoding_scope,\n",
    "                         output_fn, keep_prob):\n",
    "    train_decoder_fn = tf.contrib.seq2seq.simple_decoder_fn_train(encoder_state)\n",
    "    train_pred, _, _ = tf.contrib.seq2seq.dynamic_rnn_decoder(\n",
    "        dec_cell, train_decoder_fn, dec_embed_input, sequence_length, scope=decoding_scope)\n",
    "\n",
    "    train_logits =  output_fn(train_pred)\n",
    "    train_logits = tf.nn.dropout(train_logits, keep_prob)\n",
    "\n",
    "    return train_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def decoding_layer_infer(encoder_state, dec_cell, dec_embeddings, start_of_sequence_id,\n",
    "                         end_of_sequence_id,\n",
    "                         maximum_length, vocab_size, decoding_scope, output_fn, keep_prob):\n",
    "\n",
    "    infer_decoder_fn = tf.contrib.seq2seq.simple_decoder_fn_inference(\n",
    "        output_fn, encoder_state, dec_embeddings, start_of_sequence_id, end_of_sequence_id, \n",
    "        maximum_length - 1, vocab_size)\n",
    "    inference_logits, _, _ = tf.contrib.seq2seq.dynamic_rnn_decoder(dec_cell,\n",
    "                                                                    infer_decoder_fn,\n",
    "                                                                    scope=decoding_scope)\n",
    "    inference_logits = tf.nn.dropout(inference_logits, keep_prob)\n",
    "    return inference_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def decoding_layer(dec_embed_input, dec_embeddings, encoder_state, vocab_size, \n",
    "                   sequence_length, rnn_size,\n",
    "                   num_layers, target_vocab_to_int, keep_prob):\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "    cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=keep_prob)\n",
    "    dec_cell = tf.contrib.rnn.MultiRNNCell([cell] * num_layers)\n",
    "    \n",
    "    with tf.variable_scope('decoding') as decoding_scope:\n",
    "        output_fn = lambda x: tf.contrib.layers.fully_connected(x, vocab_size, \n",
    "                                                                tf.nn.softmax, scope=decoding_scope)\n",
    "        \n",
    "        train_logits = decoding_layer_train(encoder_state, dec_cell, \n",
    "                                            dec_embed_input, sequence_length,\n",
    "                                            decoding_scope, output_fn, keep_prob)\n",
    "        \n",
    "    with tf.variable_scope('decoding', reuse=True) as decoding_scope:\n",
    "        infer_logits = decoding_layer_infer(encoder_state, dec_cell, dec_embeddings,\n",
    "                                            source_vocab_to_int['<GO>'], \n",
    "                                            source_vocab_to_int['<EOS>'],\n",
    "                                            sequence_length, vocab_size, decoding_scope,\n",
    "                                            output_fn, keep_prob)\n",
    "\n",
    "    return train_logits, infer_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def seq2seq_model(input_data, target_data, keep_prob, batch_size, sequence_length,\n",
    "                  source_vocab_size, target_vocab_size,\n",
    "                  enc_embedding_size, dec_embedding_size, rnn_size, num_layers, \n",
    "                  target_vocab_to_int):\n",
    "    \n",
    "    enc_embed_input = tf.contrib.layers.embed_sequence(input_data, source_vocab_size,\n",
    "                                                       enc_embedding_size)\n",
    "    enc_state = encoding_layer(enc_embed_input, rnn_size, num_layers, keep_prob=keep_prob)\n",
    "    target_data = process_decoding_input(target_data, target_vocab_to_int, batch_size)\n",
    "    dec_embed = tf.Variable(tf.random_uniform([target_vocab_size, dec_embedding_size]))\n",
    "    dec_embed_input = tf.nn.embedding_lookup(dec_embed, target_data)\n",
    "\n",
    "    dec_layer = decoding_layer(dec_embed_input, dec_embed, enc_state, target_vocab_size, \n",
    "                               sequence_length,\n",
    "                               rnn_size, num_layers, target_vocab_to_int, keep_prob)\n",
    "    \n",
    "    return dec_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Neural Network Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Number of Epochs\n",
    "epochs = 1\n",
    "# Batch Size\n",
    "batch_size = 100\n",
    "# RNN Size\n",
    "rnn_size = 64\n",
    "# Number of Layers\n",
    "num_layers = 2\n",
    "# Embedding Size\n",
    "encoding_embedding_size = 128\n",
    "decoding_embedding_size = 128\n",
    "# Learning Rate\n",
    "learning_rate = 0.01\n",
    "# Dropout Keep Probability\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "save_path = 'checkpoints/dev'\n",
    "\n",
    "max_source_sentence_length = max([len(sentence) for sentence in source_int_text])\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    input_data, targets, lr, keep_prob = model_inputs()\n",
    "    sequence_length = tf.placeholder_with_default(max_source_sentence_length, \n",
    "                                                  None, name='sequence_length')\n",
    "    input_shape = tf.shape(input_data)\n",
    "    \n",
    "    train_logits, inference_logits = seq2seq_model(\n",
    "        tf.reverse(input_data, [-1]), targets, keep_prob, batch_size, sequence_length,\n",
    "        len(source_vocab_to_int), len(target_vocab_to_int),\n",
    "        encoding_embedding_size, decoding_embedding_size, rnn_size, num_layers, \n",
    "        target_vocab_to_int)\n",
    "\n",
    "    tf.identity(inference_logits, 'logits')\n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        # Loss function\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(\n",
    "            train_logits,\n",
    "            targets,\n",
    "            tf.ones([input_shape[0], sequence_length]))\n",
    "\n",
    "        # Optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "        # Gradient Clipping\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) \n",
    "                            for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch    0/100 - Train Accuracy:  0.939, Validation Accuracy:  0.930, Loss:  8.664\n",
      "Epoch   0 Batch    1/100 - Train Accuracy:  0.930, Validation Accuracy:  0.930, Loss:  8.664\n",
      "Epoch   0 Batch    2/100 - Train Accuracy:  0.939, Validation Accuracy:  0.930, Loss:  8.664\n",
      "Epoch   0 Batch    3/100 - Train Accuracy:  0.939, Validation Accuracy:  0.930, Loss:  8.664\n",
      "Epoch   0 Batch    4/100 - Train Accuracy:  0.937, Validation Accuracy:  0.930, Loss:  8.664\n",
      "Epoch   0 Batch    5/100 - Train Accuracy:  0.942, Validation Accuracy:  0.930, Loss:  8.663\n",
      "Epoch   0 Batch    6/100 - Train Accuracy:  0.946, Validation Accuracy:  0.930, Loss:  8.662\n",
      "Epoch   0 Batch    7/100 - Train Accuracy:  0.941, Validation Accuracy:  0.930, Loss:  8.658\n",
      "Epoch   0 Batch    8/100 - Train Accuracy:  0.938, Validation Accuracy:  0.930, Loss:  8.647\n",
      "Epoch   0 Batch    9/100 - Train Accuracy:  0.948, Validation Accuracy:  0.930, Loss:  8.624\n",
      "Epoch   0 Batch   10/100 - Train Accuracy:  0.939, Validation Accuracy:  0.930, Loss:  8.568\n",
      "Epoch   0 Batch   11/100 - Train Accuracy:  0.947, Validation Accuracy:  0.930, Loss:  8.449\n",
      "Epoch   0 Batch   12/100 - Train Accuracy:  0.950, Validation Accuracy:  0.930, Loss:  8.287\n",
      "Epoch   0 Batch   13/100 - Train Accuracy:  0.959, Validation Accuracy:  0.930, Loss:  8.194\n",
      "Epoch   0 Batch   14/100 - Train Accuracy:  0.950, Validation Accuracy:  0.930, Loss:  7.988\n",
      "Epoch   0 Batch   15/100 - Train Accuracy:  0.935, Validation Accuracy:  0.930, Loss:  7.961\n",
      "Epoch   0 Batch   16/100 - Train Accuracy:  0.945, Validation Accuracy:  0.930, Loss:  7.782\n",
      "Epoch   0 Batch   17/100 - Train Accuracy:  0.933, Validation Accuracy:  0.930, Loss:  7.783\n",
      "Epoch   0 Batch   18/100 - Train Accuracy:  0.949, Validation Accuracy:  0.930, Loss:  7.795\n",
      "Epoch   0 Batch   19/100 - Train Accuracy:  0.949, Validation Accuracy:  0.930, Loss:  7.862\n",
      "Epoch   0 Batch   20/100 - Train Accuracy:  0.953, Validation Accuracy:  0.930, Loss:  7.738\n",
      "Epoch   0 Batch   21/100 - Train Accuracy:  0.954, Validation Accuracy:  0.930, Loss:  7.840\n",
      "Epoch   0 Batch   22/100 - Train Accuracy:  0.946, Validation Accuracy:  0.930, Loss:  7.823\n",
      "Epoch   0 Batch   23/100 - Train Accuracy:  0.951, Validation Accuracy:  0.930, Loss:  7.897\n",
      "Epoch   0 Batch   24/100 - Train Accuracy:  0.948, Validation Accuracy:  0.930, Loss:  7.852\n",
      "Epoch   0 Batch   25/100 - Train Accuracy:  0.936, Validation Accuracy:  0.930, Loss:  7.784\n",
      "Epoch   0 Batch   26/100 - Train Accuracy:  0.945, Validation Accuracy:  0.930, Loss:  7.805\n",
      "Epoch   0 Batch   27/100 - Train Accuracy:  0.942, Validation Accuracy:  0.930, Loss:  7.897\n",
      "Epoch   0 Batch   28/100 - Train Accuracy:  0.946, Validation Accuracy:  0.930, Loss:  7.824\n",
      "Epoch   0 Batch   29/100 - Train Accuracy:  0.935, Validation Accuracy:  0.930, Loss:  7.833\n",
      "Epoch   0 Batch   30/100 - Train Accuracy:  0.962, Validation Accuracy:  0.930, Loss:  7.849\n",
      "Epoch   0 Batch   31/100 - Train Accuracy:  0.955, Validation Accuracy:  0.930, Loss:  7.797\n",
      "Epoch   0 Batch   32/100 - Train Accuracy:  0.952, Validation Accuracy:  0.930, Loss:  7.776\n",
      "Epoch   0 Batch   33/100 - Train Accuracy:  0.952, Validation Accuracy:  0.930, Loss:  7.821\n",
      "Epoch   0 Batch   34/100 - Train Accuracy:  0.946, Validation Accuracy:  0.930, Loss:  7.835\n",
      "Epoch   0 Batch   35/100 - Train Accuracy:  0.957, Validation Accuracy:  0.930, Loss:  7.963\n",
      "Epoch   0 Batch   36/100 - Train Accuracy:  0.948, Validation Accuracy:  0.930, Loss:  7.874\n",
      "Epoch   0 Batch   37/100 - Train Accuracy:  0.959, Validation Accuracy:  0.930, Loss:  7.981\n",
      "Epoch   0 Batch   38/100 - Train Accuracy:  0.946, Validation Accuracy:  0.930, Loss:  7.881\n",
      "Epoch   0 Batch   39/100 - Train Accuracy:  0.955, Validation Accuracy:  0.930, Loss:  7.959\n",
      "Epoch   0 Batch   40/100 - Train Accuracy:  0.952, Validation Accuracy:  0.930, Loss:  7.897\n",
      "Epoch   0 Batch   41/100 - Train Accuracy:  0.951, Validation Accuracy:  0.930, Loss:  7.799\n",
      "Epoch   0 Batch   42/100 - Train Accuracy:  0.962, Validation Accuracy:  0.930, Loss:  7.778\n",
      "Epoch   0 Batch   43/100 - Train Accuracy:  0.958, Validation Accuracy:  0.930, Loss:  7.886\n",
      "Epoch   0 Batch   44/100 - Train Accuracy:  0.965, Validation Accuracy:  0.930, Loss:  7.836\n",
      "Epoch   0 Batch   45/100 - Train Accuracy:  0.963, Validation Accuracy:  0.930, Loss:  7.916\n",
      "Epoch   0 Batch   46/100 - Train Accuracy:  0.956, Validation Accuracy:  0.930, Loss:  7.915\n",
      "Epoch   0 Batch   47/100 - Train Accuracy:  0.962, Validation Accuracy:  0.930, Loss:  7.848\n",
      "Epoch   0 Batch   48/100 - Train Accuracy:  0.964, Validation Accuracy:  0.930, Loss:  7.987\n",
      "Epoch   0 Batch   49/100 - Train Accuracy:  0.969, Validation Accuracy:  0.930, Loss:  7.969\n",
      "Epoch   0 Batch   50/100 - Train Accuracy:  0.955, Validation Accuracy:  0.930, Loss:  7.781\n",
      "Epoch   0 Batch   51/100 - Train Accuracy:  0.924, Validation Accuracy:  0.930, Loss:  7.894\n",
      "Epoch   0 Batch   52/100 - Train Accuracy:  0.956, Validation Accuracy:  0.930, Loss:  7.858\n",
      "Epoch   0 Batch   53/100 - Train Accuracy:  0.946, Validation Accuracy:  0.930, Loss:  7.829\n",
      "Epoch   0 Batch   54/100 - Train Accuracy:  0.953, Validation Accuracy:  0.930, Loss:  7.759\n",
      "Epoch   0 Batch   55/100 - Train Accuracy:  0.960, Validation Accuracy:  0.930, Loss:  7.851\n",
      "Epoch   0 Batch   56/100 - Train Accuracy:  0.945, Validation Accuracy:  0.930, Loss:  7.835\n",
      "Epoch   0 Batch   57/100 - Train Accuracy:  0.942, Validation Accuracy:  0.930, Loss:  7.755\n",
      "Epoch   0 Batch   58/100 - Train Accuracy:  0.939, Validation Accuracy:  0.930, Loss:  7.738\n",
      "Epoch   0 Batch   59/100 - Train Accuracy:  0.936, Validation Accuracy:  0.930, Loss:  7.786\n",
      "Epoch   0 Batch   60/100 - Train Accuracy:  0.942, Validation Accuracy:  0.930, Loss:  7.790\n",
      "Epoch   0 Batch   61/100 - Train Accuracy:  0.941, Validation Accuracy:  0.930, Loss:  7.822\n",
      "Epoch   0 Batch   62/100 - Train Accuracy:  0.944, Validation Accuracy:  0.930, Loss:  7.802\n",
      "Epoch   0 Batch   63/100 - Train Accuracy:  0.958, Validation Accuracy:  0.930, Loss:  7.796\n",
      "Epoch   0 Batch   64/100 - Train Accuracy:  0.956, Validation Accuracy:  0.930, Loss:  7.935\n",
      "Epoch   0 Batch   65/100 - Train Accuracy:  0.961, Validation Accuracy:  0.930, Loss:  7.896\n",
      "Epoch   0 Batch   66/100 - Train Accuracy:  0.955, Validation Accuracy:  0.930, Loss:  7.966\n",
      "Epoch   0 Batch   67/100 - Train Accuracy:  0.961, Validation Accuracy:  0.930, Loss:  7.908\n",
      "Epoch   0 Batch   68/100 - Train Accuracy:  0.958, Validation Accuracy:  0.930, Loss:  7.867\n",
      "Epoch   0 Batch   69/100 - Train Accuracy:  0.958, Validation Accuracy:  0.930, Loss:  7.863\n",
      "Epoch   0 Batch   70/100 - Train Accuracy:  0.950, Validation Accuracy:  0.930, Loss:  7.897\n",
      "Epoch   0 Batch   71/100 - Train Accuracy:  0.957, Validation Accuracy:  0.930, Loss:  7.893\n",
      "Epoch   0 Batch   72/100 - Train Accuracy:  0.957, Validation Accuracy:  0.930, Loss:  7.844\n",
      "Epoch   0 Batch   73/100 - Train Accuracy:  0.936, Validation Accuracy:  0.930, Loss:  7.791\n",
      "Epoch   0 Batch   74/100 - Train Accuracy:  0.937, Validation Accuracy:  0.930, Loss:  7.773\n",
      "Epoch   0 Batch   75/100 - Train Accuracy:  0.949, Validation Accuracy:  0.930, Loss:  7.762\n",
      "Epoch   0 Batch   76/100 - Train Accuracy:  0.952, Validation Accuracy:  0.930, Loss:  7.962\n",
      "Epoch   0 Batch   77/100 - Train Accuracy:  0.945, Validation Accuracy:  0.930, Loss:  7.767\n",
      "Epoch   0 Batch   78/100 - Train Accuracy:  0.940, Validation Accuracy:  0.930, Loss:  7.965\n",
      "Epoch   0 Batch   79/100 - Train Accuracy:  0.951, Validation Accuracy:  0.930, Loss:  7.879\n",
      "Epoch   0 Batch   80/100 - Train Accuracy:  0.932, Validation Accuracy:  0.930, Loss:  7.832\n",
      "Epoch   0 Batch   81/100 - Train Accuracy:  0.946, Validation Accuracy:  0.930, Loss:  7.893\n",
      "Epoch   0 Batch   82/100 - Train Accuracy:  0.957, Validation Accuracy:  0.930, Loss:  7.864\n",
      "Epoch   0 Batch   83/100 - Train Accuracy:  0.947, Validation Accuracy:  0.930, Loss:  7.832\n",
      "Epoch   0 Batch   84/100 - Train Accuracy:  0.948, Validation Accuracy:  0.930, Loss:  7.850\n",
      "Epoch   0 Batch   85/100 - Train Accuracy:  0.939, Validation Accuracy:  0.930, Loss:  7.819\n",
      "Epoch   0 Batch   86/100 - Train Accuracy:  0.942, Validation Accuracy:  0.930, Loss:  7.815\n",
      "Epoch   0 Batch   87/100 - Train Accuracy:  0.936, Validation Accuracy:  0.930, Loss:  7.848\n",
      "Epoch   0 Batch   88/100 - Train Accuracy:  0.950, Validation Accuracy:  0.930, Loss:  7.758\n",
      "Epoch   0 Batch   89/100 - Train Accuracy:  0.951, Validation Accuracy:  0.930, Loss:  7.842\n",
      "Epoch   0 Batch   90/100 - Train Accuracy:  0.956, Validation Accuracy:  0.930, Loss:  7.829\n",
      "Epoch   0 Batch   91/100 - Train Accuracy:  0.955, Validation Accuracy:  0.930, Loss:  7.793\n",
      "Epoch   0 Batch   92/100 - Train Accuracy:  0.957, Validation Accuracy:  0.930, Loss:  7.804\n",
      "Epoch   0 Batch   93/100 - Train Accuracy:  0.951, Validation Accuracy:  0.930, Loss:  7.850\n",
      "Epoch   0 Batch   94/100 - Train Accuracy:  0.955, Validation Accuracy:  0.930, Loss:  7.978\n",
      "Epoch   0 Batch   95/100 - Train Accuracy:  0.954, Validation Accuracy:  0.930, Loss:  7.820\n",
      "Epoch   0 Batch   96/100 - Train Accuracy:  0.958, Validation Accuracy:  0.930, Loss:  7.977\n",
      "Epoch   0 Batch   97/100 - Train Accuracy:  0.955, Validation Accuracy:  0.930, Loss:  7.869\n",
      "Epoch   0 Batch   98/100 - Train Accuracy:  0.958, Validation Accuracy:  0.930, Loss:  7.919\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def get_accuracy(target, logits):\n",
    "    max_seq = max(target.shape[1], logits.shape[1])\n",
    "    if max_seq - target.shape[1]:\n",
    "        target = np.pad(\n",
    "            target,\n",
    "            [(0,0),(0,max_seq - target.shape[1])],\n",
    "            'constant')\n",
    "    if max_seq - logits.shape[1]:\n",
    "        logits = np.pad(\n",
    "            logits,\n",
    "            [(0,0),(0,max_seq - logits.shape[1]), (0,0)],\n",
    "            'constant')\n",
    "\n",
    "    return np.mean(np.equal(target, np.argmax(logits, 2)))\n",
    "\n",
    "train_source = source_int_text[batch_size:]\n",
    "train_target = target_int_text[batch_size:]\n",
    "\n",
    "def pad_sentence_batch(sentence_batch):\n",
    "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
    "    return [sentence + [CODES['<PAD>']] * (max_sentence - len(sentence)) for sentence in sentence_batch]\n",
    "\n",
    "valid_source = pad_sentence_batch(source_int_text[:batch_size])\n",
    "valid_target = pad_sentence_batch(target_int_text[:batch_size])\n",
    "\n",
    "def batch_data(source, target, batch_size):\n",
    "    for batch_i in range(0, len(source)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "        source_batch = source[start_i:start_i + batch_size]\n",
    "        target_batch = target[start_i:start_i + batch_size]\n",
    "        yield np.array(pad_sentence_batch(source_batch)), np.array(pad_sentence_batch(target_batch))\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch_i in range(epochs):\n",
    "        for batch_i, (source_batch, target_batch) in enumerate(\n",
    "                batch_data(train_source, train_target, batch_size)):\n",
    "            start_time = time.time()\n",
    "            \n",
    "            _, loss = sess.run(\n",
    "                [train_op, cost],\n",
    "                {input_data: source_batch,\n",
    "                 targets: target_batch,\n",
    "                 lr: learning_rate,\n",
    "                 sequence_length: target_batch.shape[1],\n",
    "                 keep_prob: keep_probability})\n",
    "            \n",
    "            batch_train_logits = sess.run(\n",
    "                inference_logits,\n",
    "                {input_data: source_batch, keep_prob: 1.0})\n",
    "            batch_valid_logits = sess.run(\n",
    "                inference_logits,\n",
    "                {input_data: valid_source, keep_prob: 1.0})\n",
    "                \n",
    "            train_acc = get_accuracy(target_batch, batch_train_logits)\n",
    "            valid_acc = get_accuracy(np.array(valid_target), batch_valid_logits)\n",
    "            end_time = time.time()\n",
    "            print('Epoch {:>3} Batch {:>4}/{} - Train Accuracy: {:>6.3f}, Validation Accuracy: {:>6.3f}, Loss: {:>6.3f}'\n",
    "                  .format(epoch_i, batch_i, len(source_int_text) // batch_size, train_acc, valid_acc, loss))\n",
    "\n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, save_path)\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "def save_params(params):\n",
    "    with open('params.p', 'wb') as out_file:\n",
    "        pickle.dump(params, out_file)\n",
    "\n",
    "def load_params():\n",
    "    with open('params.p', mode='rb') as in_file:\n",
    "        return pickle.load(in_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "save_params(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import helper\n",
    "\n",
    "load_path = helper.load_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sentence_to_seq(sentence, vocab_to_int):\n",
    "    m = MeCab.Tagger(\"-Owakati\")\n",
    "    unk = vocab_to_int['<UNK>']\n",
    "    sentence = m.parse(sentence)\n",
    "    return [vocab_to_int.get(w, unk) for w in sentence.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input\n",
      "  Word Ids:      [5782, 2, 5620, 4508, 828, 380, 2397, 2843, 4637]\n",
      "  User Words: ['お', '<UNK>', 'の', '映画', 'は', 'あり', 'ます', 'か', '？']\n",
      "\n",
      "Prediction\n",
      "  Word Ids:      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  System Words: ['<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n"
     ]
    }
   ],
   "source": [
    "translate_sentence = 'おススメの映画はありますか？'\n",
    "\n",
    "translate_sentence = sentence_to_seq(translate_sentence, source_vocab_to_int)\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_path + '.meta')\n",
    "    loader.restore(sess, load_path)\n",
    "\n",
    "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "\n",
    "    translate_logits = sess.run(logits, {input_data: [translate_sentence], keep_prob: 1.0})[0]\n",
    "    \n",
    "print('Input')\n",
    "print('  Word Ids:      {}'.format([i for i in translate_sentence]))\n",
    "print('  User Words: {}'.format([source_int_to_vocab[i] for i in translate_sentence]))\n",
    "\n",
    "print('\\nPrediction')\n",
    "print('  Word Ids:      {}'.format([i for i in np.argmax(translate_logits, 1)]))\n",
    "print('  System Words: {}'.format([target_int_to_vocab[i] for i in np.argmax(translate_logits, 1)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def user_input(sentence):\n",
    "    sentence = sentence_to_seq(sentence, source_vocab_to_int)\n",
    "    print(sentence)\n",
    "\n",
    "    loaded_graph = tf.Graph()\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load saved model\n",
    "        loader = tf.train.import_meta_graph(load_path + '.meta')\n",
    "        loader.restore(sess, load_path)\n",
    "\n",
    "        input_data = loaded_graph.get_tensor_by_name('input:0')\n",
    "        logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "\n",
    "        translate_logits = sess.run(logits, {input_data: [sentence], keep_prob: 1.0})[0]\n",
    "        \n",
    "    return ''.join([target_int_to_vocab[i] for i in np.argmax(translate_logits, 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5950, 4834, 1165, 4985, 5714, 436, 39]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_input(\"夏といえば海だね\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "user_input(\"君は泳げるのかい？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "user_input(\"海といえば、なんですか？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "user_input(\"山はどうだい？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "user_input(\"こんにちは\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "user_input(\"元気ですか？暑くなって来ましたね？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "user_input(\"バカ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "user_input(\"君はバカ？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "user_input(\"今はどこに住んでいるのかな？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "user_input(\"オリンピックが東京で開催されるの知ってる？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "user_input(\"海はいいです\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "4px",
    "width": "254px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
